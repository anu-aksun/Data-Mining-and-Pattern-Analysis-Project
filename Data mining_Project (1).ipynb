{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "594fe862-9ea1-4be4-856e-ac155ae70d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet lxml_html_clean newspaper3k scikit-learn nltk beautifulsoup4 lxml requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acbec284-967c-4cd3-b97e-2ef7ff4c6c77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk, sys, traceback\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2ba4c83-275d-4d1c-b436-153f49c184e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from newspaper import Article\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d4cbbea-4834-44af-a437-a00f9791b26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b27bbae-7989-450c-a329-02fd204e2b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "   \n",
    "    \"https://www.bbc.com/news/articles/cx2ljgrm78zo\",       \n",
    "    \"https://www.bbc.com/news/articles/c891p1pez42o\",                 \n",
    "    \"https://www.bbc.com/news/articles/czxny96260qo\"                              \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b99f180-d3b6-4e10-a7b3-8a8f6fc35f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " urls.txt created with 3 entries\n"
     ]
    }
   ],
   "source": [
    "with open(\"urls.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(urls))\n",
    "print(\" urls.txt created with\", len(urls), \"entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "540f41f4-5734-4d33-9bb0-69b568d903fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_with_newspaper(url):\n",
    "    try:\n",
    "        a = Article(url)\n",
    "        a.download()\n",
    "        a.parse()\n",
    "        if a.text and len(a.text.split())>30:\n",
    "            return a.title or \"\", a.text\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def extract_with_bs(url, timeout=12):\n",
    "    \"\"\"Simple BeautifulSoup fallback: try to get article or paragraphs\"\"\"\n",
    "    try:\n",
    "        headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'}\n",
    "        r = requests.get(url, headers=headers, timeout=timeout)\n",
    "        r.raise_for_status()\n",
    "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "        # remove scripts/styles\n",
    "        for tag in soup([\"script\",\"style\",\"noscript\",\"header\",\"footer\",\"nav\",\"aside\"]):\n",
    "            tag.decompose()\n",
    "      \n",
    "        article_tag = soup.find(\"article\")\n",
    "        if article_tag:\n",
    "            text = article_tag.get_text(separator=\" \", strip=True)\n",
    "            if len(text.split())>30:\n",
    "                title = (soup.title.string or \"\").strip()\n",
    "                return title, text\n",
    "       \n",
    "        paragraphs = [p.get_text(separator=\" \", strip=True) for p in soup.find_all(\"p\")]\n",
    "        text = \"\\n\".join([p for p in paragraphs if p and len(p.split())>5])\n",
    "        if len(text.split())>30:\n",
    "            title = (soup.title.string or \"\").strip()\n",
    "            return title, text\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def clean_text(s):\n",
    "    s = s.lower()\n",
    "    tokens = [t for t in word_tokenize(s) if t.isalpha() and t not in STOP and len(t)>2]\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16a54a25-8bad-408d-8e57-68b7bd850f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing URL: https://www.bbc.com/news/articles/cx2ljgrm78zo\n",
      " Extracted with newspaper3k; title: Trump raises tariffs on Canadian goods in response to Reagan advert...\n",
      "\n",
      "--- Processing URL: https://www.bbc.com/news/articles/c891p1pez42o\n",
      " Extracted with newspaper3k; title: Mystery Trump supporter gives $130m to US military for paying troops during shut...\n",
      "\n",
      "--- Processing URL: https://www.bbc.com/news/articles/czxny96260qo\n",
      " Extracted with newspaper3k; title: Lassie and Lost in Space actress June Lockhart dies aged 100...\n",
      "\n",
      " Total collected documents: 3\n"
     ]
    }
   ],
   "source": [
    "collected_titles = []\n",
    "collected_raws = []\n",
    "collected_docs = []\n",
    "\n",
    "for u in urls:\n",
    "    print(\"\\n--- Processing URL:\", u)\n",
    "    \n",
    "    out = extract_with_newspaper(u)\n",
    "    if out:\n",
    "        title, text = out\n",
    "        print(\" Extracted with newspaper3k; title:\", (title[:80]+\"...\") if title else \"(no title)\")\n",
    "    else:\n",
    "        print(\" newspaper3k failed or returned little text for this URL â€” trying BeautifulSoup fallback...\")\n",
    "        out2 = extract_with_bs(u)\n",
    "        if out2:\n",
    "            title, text = out2\n",
    "            print(\" Extracted with BeautifulSoup fallback; title:\", (title[:80]+\"...\") if title else \"(no title)\")\n",
    "        else:\n",
    "            print(\" Failed to extract usable article text from this URL. Skipping.\")\n",
    "            continue\n",
    "    # store\n",
    "    collected_titles.append(title)\n",
    "    collected_raws.append(text)\n",
    "    collected_docs.append(clean_text(text))\n",
    "    time.sleep(1.0) \n",
    "\n",
    "print(\"\\n Total collected documents:\", len(collected_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c4e960b-fb69-4037-96ed-f6b86ec39923",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(collected_docs) == 0:\n",
    "    print(\"\\n No documents were collected. Common reasons:\\n\"\n",
    "          \" - URLs point to landing pages (not article pages)\\n\"\n",
    "          \" - The site uses JavaScript to render article text (newspaper/requests can't see it)\\n\"\n",
    "          \" - The site blocks scrapers or requires a login\\n\\nAdvice:\\n - Replace URLs with direct article links from BBC/Reuters/NYTimes (public),\\n - OR manually copy article text into a local .txt file and process that.\\n\")\n",
    "    raise SystemExit(\"No documents to process.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c8de2ad0-0380-4cb1-a3aa-8cd3d739c025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Topics discovered:\n",
      "  Topic 1: trade, canadian, said, trump, canada, tariffs, advert, goods\n",
      "  Topic 2: lockhart, lassie, actress, space, award, lost, film, role\n",
      "  Topic 3: military, trump, troops, donor, shutdown, make, pay, paid\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    vect = TfidfVectorizer(max_df=0.85, min_df=1, max_features=5000)\n",
    "    X = vect.fit_transform(collected_docs)\n",
    "    n_topics = min(4, max(1, len(collected_docs)))  # safe topic count\n",
    "    nmf = NMF(n_components=n_topics, random_state=0, init='nndsvda', max_iter=300)\n",
    "    nmf.fit(X)\n",
    "    H = nmf.components_\n",
    "    feat = vect.get_feature_names_out()\n",
    "    topics = [[feat[i] for i in row.argsort()[::-1][:8]] for row in H]\n",
    "    print(\"\\n Topics discovered:\")\n",
    "    for i,t in enumerate(topics,1):\n",
    "        print(f\"  Topic {i}: {', '.join(t)}\")\n",
    "except Exception as e:\n",
    "    print(\" Topic discovery failed:\", e)\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "74109fec-c773-4378-8e87-2b5ec5028cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_article(text, vect, top_n=3):\n",
    "    sents = sent_tokenize(text)\n",
    "    if len(sents) <= top_n:\n",
    "        return \" \".join(sents)\n",
    "    tf = TfidfVectorizer(vocabulary=vect.vocabulary_)\n",
    "    S = tf.fit_transform([clean_text(s) for s in sents])\n",
    "    scores = S.sum(axis=1).A1\n",
    "    top_idx = sorted(scores.argsort()[-top_n:])\n",
    "    return \" \".join([sents[i] for i in top_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6eab4b84-f42c-467b-b92a-0340989db9e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Summaries saved to summaries.csv\n"
     ]
    }
   ],
   "source": [
    "with open(\"summaries.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"title\", \"summary\"])\n",
    "    for title, raw in zip(collected_titles, collected_raws):\n",
    "        try:\n",
    "            summ = summarize_article(raw, vect)\n",
    "        except Exception as e:\n",
    "            summ = \"\"\n",
    "        writer.writerow([title, summ])\n",
    "\n",
    "print(\"\\n Summaries saved to summaries.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d5157e67-8c1c-4855-a845-c5705addc1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_article(text, vect, top_n=3):\n",
    "    sents = sent_tokenize(text)\n",
    "    if len(sents) <= top_n:\n",
    "        return \" \".join(sents)\n",
    "    tf = TfidfVectorizer(vocabulary=vect.vocabulary_)\n",
    "    S = tf.fit_transform([clean_text(s) for s in sents])\n",
    "    scores = S.sum(axis=1).A1\n",
    "    top_idx = sorted(scores.argsort()[-top_n:])\n",
    "    return \" \".join([sents[i] for i in top_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a65a2f09-80c9-4b04-b422-183c5930c979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summaries saved to 'article_summaries.txt'\n"
     ]
    }
   ],
   "source": [
    "with open(\"article_summaries.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for title, raw in zip(collected_titles, collected_raws):\n",
    "        try:\n",
    "            summ = summarize_article(raw, vect)\n",
    "        except Exception as e:\n",
    "            summ = \"( Could not summarize this article.)\"\n",
    "        f.write(f\" Title: {title}\\n\")\n",
    "        f.write(f\"Summary:\\n{summ}\\n\")\n",
    "        f.write(\"-\"*80 + \"\\n\")\n",
    "\n",
    "print(\"\\nSummaries saved to 'article_summaries.txt'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1494550-729a-4a06-8ec2-e5dd6e17611f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
